# üåç DR Infrastructure - Standalone Workspace

## Overview

This is a **completely separate Terraform workspace** dedicated to managing Disaster Recovery (DR) infrastructure. It provisions a standby EKS cluster and supporting infrastructure in a secondary AWS region (us-east-1) for production disaster recovery scenarios.

## üéØ Key Features

### Isolated Management
- **Separate Terraform State**: Independent S3 backend and DynamoDB table
- **Independent Deployment**: Deploy and manage DR without affecting primary infrastructure
- **Separate Workspace**: Complete isolation from primary infrastructure code
- **Region-Specific**: All resources deployed in DR region (us-east-1)

### Infrastructure Components
- ‚úÖ Dedicated VPC (10.1.0.0/16)
- ‚úÖ EKS Cluster with Auto Mode
- ‚úÖ AWS Load Balancer Controller
- ‚úÖ External Secrets Operator
- ‚úÖ EKS Add-ons (CoreDNS, kube-proxy, VPC CNI, EBS CSI)
- ‚úÖ IAM Roles with IRSA
- ‚úÖ Multi-AZ deployment

## üìÅ Directory Structure

```
dr-infrastructure/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ main.tf                      # DR infrastructure definition
‚îú‚îÄ‚îÄ variables.tf                 # DR-specific variables
‚îú‚îÄ‚îÄ outputs.tf                   # DR outputs
‚îú‚îÄ‚îÄ versions.tf                  # Provider versions and backend config
‚îÇ
‚îú‚îÄ‚îÄ environments/
‚îÇ   ‚îî‚îÄ‚îÄ prod/
‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfvars     # Production DR configuration
‚îÇ       ‚îî‚îÄ‚îÄ backend.conf         # Generated by setup script
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ setup-dr-prerequisites.sh  # Setup S3 backend & DynamoDB
    ‚îî‚îÄ‚îÄ deploy-dr.sh               # Deploy DR infrastructure
```

## üöÄ Quick Start

### Step 1: Setup Prerequisites

Create the S3 bucket and DynamoDB table for DR state management:

```bash
cd dr-infrastructure
./scripts/setup-dr-prerequisites.sh prod us-east-1
```

This creates:
- S3 bucket: `pipeops-terraform-state-dr-<account-id>`
- DynamoDB table: `terraform-state-lock-dr`
- Backend config: `environments/prod/backend.conf`

### Step 2: Review Configuration

Edit the DR configuration if needed:

```bash
vim environments/prod/terraform.tfvars
```

Key settings:
- `dr_region`: DR region (default: us-east-1)
- `dr_vpc_cidr`: VPC CIDR (default: 10.1.0.0/16)
- `dr_desired_capacity`: Number of nodes (default: 2 for standby)
- `dr_cluster_mode`: standby, warm, or active

### Step 3: Plan Deployment

```bash
./scripts/deploy-dr.sh prod plan
```

Review the plan to ensure it matches expectations.

### Step 4: Deploy DR Infrastructure

```bash
./scripts/deploy-dr.sh prod apply
```

This will create:
- DR VPC with subnets across 3 AZs
- EKS cluster in standby mode
- All necessary controllers and add-ons

### Step 5: Verify Deployment

```bash
# View outputs
./scripts/deploy-dr.sh prod output

# Configure kubectl
$(terraform output -raw dr_kubectl_config_command)

# Verify cluster
kubectl get nodes
kubectl get namespaces
kubectl get pods --all-namespaces
```

## üîß Configuration

### DR Cluster Modes

**Standby Mode** (Default - Cost-Optimized)
- Minimal nodes (2x t3.medium)
- Controllers running, ready to scale
- ~$243/month
- Requires scale-up during DR activation

**Warm Mode** (Balanced)
- Moderate nodes (3-4x t3.large)
- Some applications pre-deployed
- ~$400/month
- Faster activation time

**Active Mode** (Hot Standby)
- Full node count (6x t3.large)
- All applications running
- ~$600/month
- Immediate failover capability

### Modifying Configuration

```hcl
# environments/prod/terraform.tfvars

# Change cluster mode
dr_cluster_mode = "warm"  # or "active"

# Increase node count
dr_desired_capacity = 4
dr_max_capacity = 10

# Use larger instances
dr_node_instance_types = ["t3.large", "t3.xlarge"]
```

## üìä Management Commands

### Plan Changes
```bash
./scripts/deploy-dr.sh prod plan
```

### Apply Changes
```bash
./scripts/deploy-dr.sh prod apply
```

### View Outputs
```bash
./scripts/deploy-dr.sh prod output
```

### Refresh State
```bash
./scripts/deploy-dr.sh prod refresh
```

### Validate Configuration
```bash
./scripts/deploy-dr.sh prod validate
```

### Destroy DR Infrastructure
```bash
./scripts/deploy-dr.sh prod destroy
```

## üîÑ DR Activation Process

### 1. Scale Up DR Cluster

```bash
# Update terraform.tfvars
dr_desired_capacity = 6
dr_cluster_mode = "active"

# Apply changes
./scripts/deploy-dr.sh prod apply
```

### 2. Configure kubectl

```bash
aws eks update-kubeconfig --region us-east-1 --name pipeops-prod-dr-eks
```

### 3. Deploy Applications

```bash
# Apply Kubernetes manifests
kubectl apply -k ../k8s-manifests/overlays/prod

# Or use ArgoCD
kubectl port-forward svc/argocd-server -n argocd 8080:443
argocd app sync --all
```

### 4. Update DNS

Point your DNS records to the DR region load balancers.

### 5. Verify Services

```bash
kubectl get pods --all-namespaces
kubectl get svc,ingress --all-namespaces
```

## üí∞ Cost Management

### Monthly Costs (Standby Mode)

| Component | Configuration | Cost |
|-----------|--------------|------|
| EKS Cluster | 1 cluster | $73 |
| EC2 Nodes | 2x t3.medium | $60 |
| NAT Gateways | 3 AZs | $100 |
| Data Transfer | Minimal | $10 |
| **Total** | **Standby** | **~$243/month** |

### Cost Optimization Tips

1. **Reduce NAT Gateways**: Use single NAT Gateway
   ```bash
   # Modify modules/vpc to use single_nat_gateway = true
   ```

2. **Scale Down Off-Hours**: Reduce nodes during low-risk periods
   ```bash
   dr_desired_capacity = 1  # Minimum viable
   ```

3. **Use Spot Instances**: For non-critical workloads
   ```bash
   # Add spot instance support in node configuration
   ```

4. **Review Regularly**: Monthly cost reviews
   ```bash
   aws ce get-cost-and-usage --time-period Start=2024-01-01,End=2024-01-31 \
     --granularity MONTHLY --metrics BlendedCost \
     --filter file://dr-filter.json
   ```

## üîê Security

### IAM Roles
- Separate IAM roles for DR cluster
- IRSA for fine-grained permissions
- Cross-region access for Secrets Manager

### Network Security
- Isolated VPC (10.1.0.0/16)
- Private subnets for nodes
- Security groups with least privilege
- NAT Gateways for controlled egress

### Secrets Management
```bash
# Replicate secrets to DR region
aws secretsmanager replicate-secret-to-regions \
  --secret-id pipeops-prod-db-credentials \
  --add-replica-regions Region=us-east-1 \
  --region us-west-2
```

## üìà Monitoring

### CloudWatch Metrics

```bash
# View DR cluster metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/EKS \
  --metric-name cluster_failed_node_count \
  --dimensions Name=ClusterName,Value=pipeops-prod-dr-eks \
  --region us-east-1 \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Average
```

### Kubernetes Monitoring

```bash
# Check cluster health
kubectl get nodes
kubectl top nodes
kubectl get pods --all-namespaces

# Check events
kubectl get events --all-namespaces --sort-by='.lastTimestamp'
```

## üß™ Testing

### Regular DR Drills (Quarterly)

```bash
# 1. Scale up DR cluster
dr_desired_capacity = 4
./scripts/deploy-dr.sh prod apply

# 2. Deploy test application
kubectl apply -f test-app.yaml

# 3. Verify connectivity
kubectl port-forward svc/test-app 8080:80
curl localhost:8080

# 4. Scale back down
dr_desired_capacity = 2
./scripts/deploy-dr.sh prod apply

# 5. Clean up
kubectl delete -f test-app.yaml
```

## üö® Troubleshooting

### Issue: Backend Not Initialized

**Error**: `Backend configuration not found`

**Solution**:
```bash
./scripts/setup-dr-prerequisites.sh prod us-east-1
```

### Issue: Cannot Access DR Cluster

**Error**: `kubectl` cannot connect

**Solution**:
```bash
# Update kubeconfig
aws eks update-kubeconfig --region us-east-1 --name pipeops-prod-dr-eks

# Verify AWS credentials
aws sts get-caller-identity

# Check cluster status
aws eks describe-cluster --name pipeops-prod-dr-eks --region us-east-1
```

### Issue: Terraform State Lock

**Error**: `Error locking state`

**Solution**:
```bash
# Force unlock (use with caution)
terraform force-unlock <lock-id>

# Or check DynamoDB for stuck locks
aws dynamodb scan --table-name terraform-state-lock-dr --region us-east-1
```

## üìö Related Documentation

- [../RDS_COMPLETE_GUIDE.md](../RDS_COMPLETE_GUIDE.md) - RDS DR setup
- [../DR_EKS_CLUSTER_GUIDE.md](../DR_EKS_CLUSTER_GUIDE.md) - DR architecture details
- [../ENVIRONMENT_DEPLOYMENT_GUIDE.md](../ENVIRONMENT_DEPLOYMENT_GUIDE.md) - Primary deployment
- [../QUICK_START.md](../QUICK_START.md) - Quick reference

## üéì Best Practices

1. **Separate State Management**: Never mix DR and primary state
2. **Regular Testing**: Quarterly DR drills
3. **Documentation**: Keep runbooks updated
4. **Monitoring**: Set up CloudWatch alarms
5. **Cost Reviews**: Monthly cost analysis
6. **Security Audits**: Regular security reviews
7. **Automation**: Use scripts for consistency

## üîÑ Maintenance Schedule

### Weekly
- [ ] Check DR cluster health
- [ ] Verify kubectl access

### Monthly
- [ ] Review costs
- [ ] Check for Terraform updates
- [ ] Review CloudWatch logs

### Quarterly
- [ ] Perform DR drill
- [ ] Update documentation
- [ ] Review IAM policies
- [ ] Test RDS failover

### Annual
- [ ] Full DR simulation
- [ ] Security audit
- [ ] Cost optimization review
- [ ] Update DR procedures

## ü§ù Integration with Primary Infrastructure

### Consuming Primary Workspace Outputs

The DR workspace can consume outputs from the primary workspace using Terraform remote state or data sources.

#### Option 1: Using Remote State (Recommended)

```hcl
# In dr-infrastructure/main.tf
data "terraform_remote_state" "primary" {
  backend = "s3"
  
  config = {
    bucket         = "pipeops-prod-terraform-state"  # Primary workspace bucket
    key            = "prod/terraform.tfstate"
    region         = "us-west-2"  # Primary region
    dynamodb_table = "pipeops-prod-terraform-locks"
  }
}

# Use the KMS key ARN from primary workspace
resource "aws_kms_key" "dr_rds" {
  # Use primary's DR KMS key if available, otherwise create new
  kms_key_id = data.terraform_remote_state.primary.outputs.rds_dr_kms_key_arn != null ? 
    data.terraform_remote_state.primary.outputs.rds_dr_kms_key_arn : 
    aws_kms_key.dr_rds_new[0].arn
}
```

#### Option 2: Using Data Source (If KMS Key ARN is Known)

```hcl
# In dr-infrastructure/main.tf
data "aws_kms_key" "primary_dr_backup_key" {
  # Get the KMS key created by primary workspace for cross-region backups
  # ARN format: arn:aws:kms:us-east-1:ACCOUNT_ID:key/KEY_ID
  # Get this from primary workspace output: terraform output -raw rds_dr_kms_key_arn
  key_id = "alias/pipeops-prod-rds-dr"  # Or use full ARN
}

# Use it in your DR RDS configuration
resource "aws_db_instance" "dr_replica" {
  # ... other config ...
  kms_key_id = data.aws_kms_key.primary_dr_backup_key.arn
}
```

#### Available Primary Workspace Outputs

After deploying the primary workspace, you can access:

```bash
# Get KMS key ARN for DR region backups
terraform output -raw rds_dr_kms_key_arn

# Get KMS key ID
terraform output -raw rds_dr_kms_key_id

# Get primary RDS ARN (needed for DR replica)
terraform output -raw rds_arn
```

**Note**: The `rds_dr_kms_key_arn` and `rds_dr_kms_key_id` outputs are available when:
- `enable_cross_region_backups = true` (for backup replication), OR
- `enable_cross_region_dr = true` (for DR replica)

This allows the DR workspace to reuse the same KMS key created by the primary workspace, avoiding key proliferation and ensuring consistent encryption.

### RDS Connection

The DR EKS cluster can connect to the DR RDS replica:

```yaml
# In your application deployment
env:
  - name: DB_HOST
    valueFrom:
      secretKeyRef:
        name: db-credentials
        key: dr_endpoint  # Points to DR RDS replica
```

### Secrets Replication

Ensure secrets are replicated to DR region:

```bash
# List replicated secrets
aws secretsmanager list-secrets \
  --region us-east-1 \
  --query 'SecretList[?ReplicationStatus].Name'
```

### Cross-Region Networking

For advanced scenarios, consider:
- VPC Peering between primary and DR regions
- Transit Gateway for multi-region connectivity
- AWS PrivateLink for service-to-service communication

## üìû Support

For issues or questions:
1. Check troubleshooting section above
2. Review Terraform logs: `terraform show`
3. Check AWS CloudWatch logs
4. Review related documentation

---

**Note**: This DR workspace is designed for production use only. It provides a completely isolated environment for managing disaster recovery infrastructure independently from the primary infrastructure.
