# ğŸŒ DR Infrastructure - Standalone Workspace

## Overview

This is a **completely separate Terraform workspace** dedicated to managing Disaster Recovery (DR) infrastructure. It provisions a standby EKS cluster and supporting infrastructure in a secondary AWS region (us-east-1) for production disaster recovery scenarios.

## ğŸ¯ Key Features

### Isolated Management
- **Separate Terraform State**: Independent S3 backend and DynamoDB table
- **Independent Deployment**: Deploy and manage DR without affecting primary infrastructure
- **Separate Workspace**: Complete isolation from primary infrastructure code
- **Region-Specific**: All resources deployed in DR region (us-east-1)

### Infrastructure Components
- âœ… Dedicated VPC (10.1.0.0/16)
- âœ… EKS Cluster with Auto Mode
- âœ… AWS Load Balancer Controller
- âœ… External Secrets Operator
- âœ… EKS Add-ons (CoreDNS, kube-proxy, VPC CNI, EBS CSI)
- âœ… IAM Roles with IRSA
- âœ… Multi-AZ deployment

## ğŸ“ Directory Structure

```
dr-infrastructure/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ main.tf                      # DR infrastructure definition
â”œâ”€â”€ variables.tf                 # DR-specific variables
â”œâ”€â”€ outputs.tf                   # DR outputs
â”œâ”€â”€ versions.tf                  # Provider versions and backend config
â”‚
â”œâ”€â”€ environments/
â”‚   â””â”€â”€ prod/
â”‚       â”œâ”€â”€ terraform.tfvars     # Production DR configuration
â”‚       â””â”€â”€ backend.conf         # Generated by setup script
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ setup-dr-prerequisites.sh  # Setup S3 backend & DynamoDB
    â””â”€â”€ deploy-dr.sh               # Deploy DR infrastructure
```

## ğŸš€ Quick Start

### Step 1: Setup Prerequisites

Create the S3 bucket and DynamoDB table for DR state management:

```bash
cd dr-infrastructure
./scripts/setup-dr-prerequisites.sh prod us-east-1
```

This creates:
- S3 bucket: `pipeops-terraform-state-dr-<account-id>`
- DynamoDB table: `terraform-state-lock-dr`
- Backend config: `environments/prod/backend.conf`

### Step 2: Review Configuration

Edit the DR configuration if needed:

```bash
vim environments/prod/terraform.tfvars
```

Key settings:
- `dr_region`: DR region (default: us-east-1)
- `dr_vpc_cidr`: VPC CIDR (default: 10.1.0.0/16)
- `dr_desired_capacity`: Number of nodes (default: 2 for standby)
- `dr_cluster_mode`: standby, warm, or active

### Step 3: Plan Deployment

```bash
./scripts/deploy-dr.sh prod plan
```

Review the plan to ensure it matches expectations.

### Step 4: Deploy DR Infrastructure

```bash
./scripts/deploy-dr.sh prod apply
```

This will create:
- DR VPC with subnets across 3 AZs
- EKS cluster in standby mode
- All necessary controllers and add-ons

### Step 5: Verify Deployment

```bash
# View outputs
./scripts/deploy-dr.sh prod output

# Configure kubectl
$(terraform output -raw dr_kubectl_config_command)

# Verify cluster
kubectl get nodes
kubectl get namespaces
kubectl get pods --all-namespaces
```

## ğŸ”§ Configuration

### DR Cluster Modes

**Standby Mode** (Default - Cost-Optimized)
- Minimal nodes (2x t3.medium)
- Controllers running, ready to scale
- ~$243/month
- Requires scale-up during DR activation

**Warm Mode** (Balanced)
- Moderate nodes (3-4x t3.large)
- Some applications pre-deployed
- ~$400/month
- Faster activation time

**Active Mode** (Hot Standby)
- Full node count (6x t3.large)
- All applications running
- ~$600/month
- Immediate failover capability

### Modifying Configuration

```hcl
# environments/prod/terraform.tfvars

# Change cluster mode
dr_cluster_mode = "warm"  # or "active"

# Increase node count
dr_desired_capacity = 4
dr_max_capacity = 10

# Use larger instances
dr_node_instance_types = ["t3.large", "t3.xlarge"]
```

## ğŸ“Š Management Commands

### Plan Changes
```bash
./scripts/deploy-dr.sh prod plan
```

### Apply Changes
```bash
./scripts/deploy-dr.sh prod apply
```

### View Outputs
```bash
./scripts/deploy-dr.sh prod output
```

### Refresh State
```bash
./scripts/deploy-dr.sh prod refresh
```

### Validate Configuration
```bash
./scripts/deploy-dr.sh prod validate
```

### Destroy DR Infrastructure
```bash
./scripts/deploy-dr.sh prod destroy
```

## ğŸ”„ DR Activation Process

### 1. Scale Up DR Cluster

```bash
# Update terraform.tfvars
dr_desired_capacity = 6
dr_cluster_mode = "active"

# Apply changes
./scripts/deploy-dr.sh prod apply
```

### 2. Configure kubectl

```bash
aws eks update-kubeconfig --region us-east-1 --name pipeops-prod-dr-eks
```

### 3. Deploy Applications

```bash
# Apply Kubernetes manifests
kubectl apply -k ../k8s-manifests/overlays/prod

# Or use ArgoCD
kubectl port-forward svc/argocd-server -n argocd 8080:443
argocd app sync --all
```

### 4. Update DNS

Point your DNS records to the DR region load balancers.

### 5. Verify Services

```bash
kubectl get pods --all-namespaces
kubectl get svc,ingress --all-namespaces
```

## ğŸ’° Cost Management

### Monthly Costs (Standby Mode)

| Component | Configuration | Cost |
|-----------|--------------|------|
| EKS Cluster | 1 cluster | $73 |
| EC2 Nodes | 2x t3.medium | $60 |
| NAT Gateways | 3 AZs | $100 |
| Data Transfer | Minimal | $10 |
| **Total** | **Standby** | **~$243/month** |

### Cost Optimization Tips

1. **Reduce NAT Gateways**: Use single NAT Gateway
   ```bash
   # Modify modules/vpc to use single_nat_gateway = true
   ```

2. **Scale Down Off-Hours**: Reduce nodes during low-risk periods
   ```bash
   dr_desired_capacity = 1  # Minimum viable
   ```

3. **Use Spot Instances**: For non-critical workloads
   ```bash
   # Add spot instance support in node configuration
   ```

4. **Review Regularly**: Monthly cost reviews
   ```bash
   aws ce get-cost-and-usage --time-period Start=2024-01-01,End=2024-01-31 \
     --granularity MONTHLY --metrics BlendedCost \
     --filter file://dr-filter.json
   ```

## ğŸ” Security

### IAM Roles
- Separate IAM roles for DR cluster
- IRSA for fine-grained permissions
- Cross-region access for Secrets Manager

### Network Security
- Isolated VPC (10.1.0.0/16)
- Private subnets for nodes
- Security groups with least privilege
- NAT Gateways for controlled egress

### Secrets Management
```bash
# Replicate secrets to DR region
aws secretsmanager replicate-secret-to-regions \
  --secret-id pipeops-prod-db-credentials \
  --add-replica-regions Region=us-east-1 \
  --region us-west-2
```

## ğŸ“ˆ Monitoring

### CloudWatch Metrics

```bash
# View DR cluster metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/EKS \
  --metric-name cluster_failed_node_count \
  --dimensions Name=ClusterName,Value=pipeops-prod-dr-eks \
  --region us-east-1 \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Average
```

### Kubernetes Monitoring

```bash
# Check cluster health
kubectl get nodes
kubectl top nodes
kubectl get pods --all-namespaces

# Check events
kubectl get events --all-namespaces --sort-by='.lastTimestamp'
```

## ğŸ§ª Testing

### Regular DR Drills (Quarterly)

```bash
# 1. Scale up DR cluster
dr_desired_capacity = 4
./scripts/deploy-dr.sh prod apply

# 2. Deploy test application
kubectl apply -f test-app.yaml

# 3. Verify connectivity
kubectl port-forward svc/test-app 8080:80
curl localhost:8080

# 4. Scale back down
dr_desired_capacity = 2
./scripts/deploy-dr.sh prod apply

# 5. Clean up
kubectl delete -f test-app.yaml
```

## ğŸš¨ Troubleshooting

### Issue: Backend Not Initialized

**Error**: `Backend configuration not found`

**Solution**:
```bash
./scripts/setup-dr-prerequisites.sh prod us-east-1
```

### Issue: Cannot Access DR Cluster

**Error**: `kubectl` cannot connect

**Solution**:
```bash
# Update kubeconfig
aws eks update-kubeconfig --region us-east-1 --name pipeops-prod-dr-eks

# Verify AWS credentials
aws sts get-caller-identity

# Check cluster status
aws eks describe-cluster --name pipeops-prod-dr-eks --region us-east-1
```

### Issue: Terraform State Lock

**Error**: `Error locking state`

**Solution**:
```bash
# Force unlock (use with caution)
terraform force-unlock <lock-id>

# Or check DynamoDB for stuck locks
aws dynamodb scan --table-name terraform-state-lock-dr --region us-east-1
```

## ğŸ“š Related Documentation

- [../RDS_COMPLETE_GUIDE.md](../RDS_COMPLETE_GUIDE.md) - RDS DR setup
- [../DR_EKS_CLUSTER_GUIDE.md](../DR_EKS_CLUSTER_GUIDE.md) - DR architecture details
- [../ENVIRONMENT_DEPLOYMENT_GUIDE.md](../ENVIRONMENT_DEPLOYMENT_GUIDE.md) - Primary deployment
- [../QUICK_START.md](../QUICK_START.md) - Quick reference

## ğŸ“ Best Practices

1. **Separate State Management**: Never mix DR and primary state
2. **Regular Testing**: Quarterly DR drills
3. **Documentation**: Keep runbooks updated
4. **Monitoring**: Set up CloudWatch alarms
5. **Cost Reviews**: Monthly cost analysis
6. **Security Audits**: Regular security reviews
7. **Automation**: Use scripts for consistency

## ğŸ”„ Maintenance Schedule

### Weekly
- [ ] Check DR cluster health
- [ ] Verify kubectl access

### Monthly
- [ ] Review costs
- [ ] Check for Terraform updates
- [ ] Review CloudWatch logs

### Quarterly
- [ ] Perform DR drill
- [ ] Update documentation
- [ ] Review IAM policies
- [ ] Test RDS failover

### Annual
- [ ] Full DR simulation
- [ ] Security audit
- [ ] Cost optimization review
- [ ] Update DR procedures

## ğŸ¤ Integration with Primary Infrastructure

### RDS Connection

The DR EKS cluster can connect to the DR RDS replica:

```yaml
# In your application deployment
env:
  - name: DB_HOST
    valueFrom:
      secretKeyRef:
        name: db-credentials
        key: dr_endpoint  # Points to DR RDS replica
```

### Secrets Replication

Ensure secrets are replicated to DR region:

```bash
# List replicated secrets
aws secretsmanager list-secrets \
  --region us-east-1 \
  --query 'SecretList[?ReplicationStatus].Name'
```

### Cross-Region Networking

For advanced scenarios, consider:
- VPC Peering between primary and DR regions
- Transit Gateway for multi-region connectivity
- AWS PrivateLink for service-to-service communication

## ğŸ“ Support

For issues or questions:
1. Check troubleshooting section above
2. Review Terraform logs: `terraform show`
3. Check AWS CloudWatch logs
4. Review related documentation

---

**Note**: This DR workspace is designed for production use only. It provides a completely isolated environment for managing disaster recovery infrastructure independently from the primary infrastructure.
